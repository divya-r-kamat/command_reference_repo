{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "In real-world datasets, it is fairly common to have columns that are nothing but noise. Modern day datasets are very rich in information with data collected from millions of IoT devices and sensors. This makes the data high dimensional and it is quite common to see datasets with hundreds of features and is not unusual to see it go to tens of thousands.\n",
    "\n",
    "You are better off getting rid of such variables because of\n",
    "- the memory space they occupy, the time and the computational resources it is going to cost, especially in large datasets.\n",
    "- Models have increasing risk of overfitting with increasing number of features.\n",
    "\n",
    "Sometimes, you have a variable that makes business sense, but you are not sure if it actually helps in predicting the Y. You also need to consider the fact that, a feature that could be useful in one ML algorithm (say a regression) may go underrepresented or unused by another (like a KNN model).\n",
    "\n",
    "Having said that, it is still possible that a variable that shows poor signs of helping to explain the response variable (Y), can turn out to be significantly useful in the presence of (or combination with) other predictors. What I mean by that is, a variable might have a low correlation value of (~0.2) with Y. But in the presence of other variables, it can help to explain certain patterns/phenomenon that other variables can’t explain.\n",
    "\n",
    "In such cases, it can be hard to make a call whether to include or exclude such variables.\n",
    "\n",
    "The strategies we are about to discuss can help fix such problems. Not only that, it will also help understand if a particular variable is important or not and how much it is contributing to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Selection\n",
    "\n",
    "Filter Methods considers the relationship between features and the target variable to compute the importance of features.\n",
    "\n",
    "Scikit learn provides the Selecting K best features using F-Test.\n",
    "\n",
    "    - Regression Tasks (sklearn.feature_selection.f_regression)\n",
    "    - Classification Tasks (sklearn.feature_selection.f_classif)\n",
    "\n",
    "#### Drawbacks of using F-Test to select your features. \n",
    "F-Test checks for and only captures linear relationships between features and labels. A highly correlated feature is given higher score and less correlated features are given lower score.\n",
    "\n",
    "Correlation is highly deceptive as it doesn’t capture strong non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv(\"D://Blogs//train.csv\")\n",
    "X = data.iloc[:,0:20]  #independent columns\n",
    "y = data.iloc[:,-1]    #target column i.e price range\n",
    "#apply SelectKBest class to extract top 10 best features\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=10)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(10,'Score'))  #print 10 best features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information\n",
    "\n",
    "Mutual Information between two variables measures the dependence of one variable to another. If X and Y are two variables, and\n",
    "\n",
    "- If X and Y are independent, then no information about Y can be obtained by knowing X or vice versa. Hence their mutual information is 0.\n",
    "- If X is a deterministic function of Y, then we can determine X from Y and Y from X with mutual information 1.\n",
    "- When we have Y = f(X,Z,M,N), 0 < mutual information < 1\n",
    "We can select our features from feature space by ranking their mutual information with the target variable.\n",
    "\n",
    "Advantage of using mutual information over F-Test is, it does well with the non-linear relationship between feature and target variable.\n",
    "\n",
    "Sklearn offers feature selection with Mutual Information for regression and classification tasks.\n",
    "\n",
    "- sklearn.feature_selection.mututal_info_regression \n",
    "- sklearn.feature_selection.mututal_info_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Threshold\n",
    "This method removes features with variation below a certain cutoff.\n",
    "\n",
    "The idea is when a feature doesn’t vary much within itself, it generally has very little predictive power.\n",
    "\n",
    "    sklearn.feature_selection.VarianceThreshold\n",
    "Variance Threshold doesn’t consider the relationship of features with the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the iris datasets\n",
    "dataset = datasets.load_iris()\n",
    "# fit an Extra Trees model to the data\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(dataset.data, dataset.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb8AAAD8CAYAAADnqKoEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x250d59fb4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data = pd.read_csv(\"D://Blogs//train.csv\")\n",
    "# X = data.iloc[:,0:20]  #independent columns\n",
    "# y = data.iloc[:,-1]    #target column i.e price range\n",
    "\n",
    "#use inbuilt class feature_importances of tree based classifiers\n",
    "dfscores = pd.DataFrame(model.feature_importances_)\n",
    "dfcolumns = pd.DataFrame(list(dataset.feature_names))\n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Columns','Importance']\n",
    "featureScores.sort_values(['Importance','Columns'],ascending = [False,True])\n",
    "\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=dataset.feature_names)\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix with Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import seaborn as sns\n",
    "# data = pd.read_csv(\"D://Blogs//train.csv\")\n",
    "# X = data.iloc[:,0:20]  #independent columns\n",
    "# y = data.iloc[:,-1]    #target column i.e price range\n",
    "# #get correlations of each features in dataset\n",
    "# corrmat = data.corr()\n",
    "# top_corr_features = corrmat.index\n",
    "# plt.figure(figsize=(20,20))\n",
    "# #plot heat map\n",
    "# g=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper Methods\n",
    "Wrapper Methods generate models with a subsets of feature and gauge their model performances.\n",
    "\n",
    "### Forward Search\n",
    "This method allows you to search for the best feature w.r.t model performance and add them to your feature subset one after the other.\n",
    "\n",
    "For data with n features,\n",
    "\n",
    "->On first round ‘n’ models are created with individual feature and the best predictive feature is selected.\n",
    "\n",
    "->On second round, ‘n-1’ models are created with each feature and the previously selected feature.\n",
    "\n",
    "->This is repeated till a best subset of ‘m’ features are selected.\n",
    "\n",
    "### Recursive Feature Elimination\n",
    "The Recursive Feature Elimination (RFE) method is a feature selection approach.  This method eliminates worst performing features on a particular model one after the other until the best subset of features are known. It works by recursively removing attributes and building a model on those attributes that remain. It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.\n",
    "\n",
    "For data with n features,\n",
    "\n",
    "->On first round ‘n-1’ models are created with combination of all features except one. The least performing feature is removed\n",
    "\n",
    "-> On second round ‘n-2’ models are created by removing another feature.\n",
    "\n",
    "Wrapper Methods promises you a best set of features with a extensive greedy search.\n",
    "\n",
    "#### Drawbacks of wrapper methods \n",
    "The sheer amount of models that needs to be trained. It is computationally very expensive and is infeasible with large number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the iris datasets\n",
    "dataset = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a base classifier used to evaluate a subset of attributes\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the RFE model and select 3 attributes\n",
    "rfe = RFE(model, 3)\n",
    "rfe = rfe.fit(dataset.data, dataset.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True  True  True]\n",
      "[2 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# summarize the selection of the attributes\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dataset.feature_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
